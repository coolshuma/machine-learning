{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis and spam filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(a, fname):\n",
    "    with open('{}.txt'.format(fname), 'w') as f:\n",
    "        try:\n",
    "            for elem in a:\n",
    "                f.write(str(elem)+ ' ')\n",
    "        except:\n",
    "            f.write(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pipeline(vectorizer, clf):\n",
    "    return Pipeline(steps=[\n",
    "    ('vectorizer', vectorizer), \n",
    "    ('classifier', clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pd.read_table('SMSSpamCollection.txt', header=None)\n",
    "labels = map(lambda x: 1 if x == 'spam' else 0, texts[0])\n",
    "texts = list(texts.drop(columns=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "texts_features = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "np.random.seed(2)\n",
    "clf = LogisticRegression()\n",
    "score = cross_val_score(clf, X=texts_features, y=labels, cv=10, \n",
    "                        scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print np.round(score.mean(), decimals=1)\n",
    "save_txt(np.round(score.mean(), 1), 'a1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "score = cross_val_score(get_pipeline(CountVectorizer(), LogisticRegression()), \n",
    "                        X=texts, y=labels, cv=10, scoring='f1')\n",
    "print np.round(score.mean(), decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\", \n",
    "\"FreeMsg: Txt: claim your reward of 3 hours talk time\", \"Have you visited the last lecture on physics?\", \n",
    "\"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\", \"Only 99$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "clf.fit(texts_features, labels)\n",
    "test_features = vectorizer.transform(test)\n",
    "predicts = clf.predict(test_features)\n",
    "print predicts\n",
    "save_txt(predicts, 'a2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranges = [(2,2), (3,3), (1,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_results = []\n",
    "for cur_range in ranges:\n",
    "    score = cross_val_score(get_pipeline(CountVectorizer(ngram_range=cur_range), \n",
    "                                         LogisticRegression()), \n",
    "                            X=texts, y=labels, cv=10, scoring='f1')\n",
    "    ngram_results.append(np.round(score.mean(), decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81999999999999995, 0.72999999999999998, 0.93000000000000005]\n"
     ]
    }
   ],
   "source": [
    "print ngram_results\n",
    "save_txt(ngram_results, 'a3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use naive Bayes and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "ngram_results = []\n",
    "for cur_range in ranges:\n",
    "    texts_features = \\\n",
    "        CountVectorizer(ngram_range=cur_range).fit_transform(texts)\n",
    "    clf = MultinomialNB()\n",
    "    score = cross_val_score(clf, X=texts_features, y=labels, \n",
    "                            cv=10, scoring='f1')\n",
    "    ngram_results.append(np.round(score.mean(), decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65000000000000002, 0.38, 0.89000000000000001]\n"
     ]
    }
   ],
   "source": [
    "print ngram_results\n",
    "save_txt(ngram_results, 'a4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogReg again and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_score = cross_val_score(get_pipeline(CountVectorizer(), LogisticRegression()), \n",
    "                        X=texts, y=labels, cv=10, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_score = cross_val_score(get_pipeline(TfidfVectorizer(), LogisticRegression()), \n",
    "                        X=texts, y=labels, cv=10, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933 0.879\n"
     ]
    }
   ],
   "source": [
    "print np.around(count_vect_score, 3), np.around(tfidf_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_txt(-1, 'a5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
