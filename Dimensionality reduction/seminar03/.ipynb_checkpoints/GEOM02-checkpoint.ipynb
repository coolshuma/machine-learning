{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import numpy.random as rand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список данных из статьи:\n",
    "1. сферы\n",
    "* Swiss roll \n",
    "* moons\n",
    "* s_curve\n",
    "* Airfoils \n",
    "* MNIST\n",
    "* iris dataset \n",
    "* diabetes dataset\n",
    "* boston house-prices dataset \n",
    "* Olivetti faces data-set from AT&T\n",
    "* California housing dataset \n",
    "* Labeled Faces in the Wild (LFW) people dataset\n",
    "\n",
    "Методы: \n",
    "1. Левиной-Бикеля\n",
    "* Isomap\n",
    "* Гранаты\n",
    "\n",
    "Вопросы:\n",
    "1. Как считать residual variance в [Isomap](http://web.mit.edu/cocosci/Papers/sci_reprint.pdf)?\n",
    "* MNIST какой брать (28*28 или из sklearn)? и как брать (все слишком долго считается)? в почте\n",
    "* Как генерить гиперкубы, мёбиуса, может что-то ещё? не надо\n",
    "* Надо ли к сгенерированным данным добавлять более многомерный шум? как это сделать правильно? нет\n",
    "\n",
    "Сделать:\n",
    "1. для сфер поиграться с r_max в гранате\n",
    "* сделать Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-30D spheres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# generate shperes of different dimensions (from 2 to 30)\n",
    "n, d_max = 1000, 30\n",
    "d_sphere_data = []\n",
    "for d in range(2,d_max+1):\n",
    "    norm_x = rand.randn(n,d)\n",
    "    sphere_data = pd.DataFrame(norm_x/np.sqrt(np.sum(norm_x**2, axis=1))[:,None])\n",
    "    d_sphere_data.append(sphere_data)\n",
    "print('Number of objects = ', n)\n",
    "print('Number of features = 2-30')\n",
    "\n",
    "# plot 3d sphere\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter3D(norm_x[:,0],norm_x[:,1],norm_x[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# generate swiss roll and swiss roll with noise\n",
    "n,noise = 200,1\n",
    "swiss_roll_data = make_swiss_roll(n)[0]\n",
    "swiss_roll_with_noise_data = make_swiss_roll(n, noise)[0]\n",
    "\n",
    "# plot noise version of swiss roll\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(swiss_roll_with_noise_data[:,0],swiss_roll_with_noise_data[:,1],swiss_roll_with_noise_data[:,2])\n",
    "swiss_roll_data = pd.DataFrame(swiss_roll_data)\n",
    "swiss_roll_with_noise_data = pd.DataFrame(swiss_roll_with_noise_data)\n",
    "print('Number of objects = ', n)\n",
    "print('Number of features = ',swiss_roll_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# generate moons and moons with noise\n",
    "n,noise = 200,0.1\n",
    "moons_data = np.zeros((n,3))\n",
    "moons_data[:,0:2] = make_moons(n)[0]\n",
    "moons_data[:,2] = rand.uniform(0,1,200)\n",
    "\n",
    "moons_with_noise_data = np.zeros((n,3))\n",
    "moons_with_noise_data[:,0:2] = make_moons(n,noise=noise)[0]\n",
    "moons_with_noise_data[:,2] = rand.uniform(0,1,200)\n",
    "\n",
    "# plot noise version of moons\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(moons_with_noise_data[:,0],moons_with_noise_data[:,1],moons_with_noise_data[:,2])\n",
    "moons_data = pd.DataFrame(moons_data)\n",
    "moons_with_noise_data = pd.DataFrame(moons_with_noise_data)\n",
    "print('Number of objects = ', n)\n",
    "print('Number of features = ',moons_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_s_curve\n",
    "\n",
    "# generate s curve and s curve with noise\n",
    "n,noise = 200,0.2\n",
    "s_curve_data = make_s_curve(n)[0]\n",
    "s_curve_with_noise_data = make_s_curve(n,noise=noise)[0]\n",
    "\n",
    "# plot noise version of s curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter3D(s_curve_with_noise_data[:,0],s_curve_with_noise_data[:,1],s_curve_with_noise_data[:,2])\n",
    "s_curve_data = pd.DataFrame(moons_data)\n",
    "s_curve_with_noise_data = pd.DataFrame(moons_data)\n",
    "print('Number of objects = ', n)\n",
    "print('Number of features = ',s_curve_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airfoils_data = pd.read_csv('airfoils.csv')\n",
    "print('Number of objects = ', airfoils_data.shape[0])\n",
    "print('Number of features = ',airfoils_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mnist_data = pd.read_csv('mnist.csv')\n",
    "small_mnist_data = mnist_data.groupby('label').head(100).reset_index(drop=True)\n",
    "mnist_data = mnist_data.drop('label',axis = 1)\n",
    "small_mnist_data\n",
    "small_mnist_data = small_mnist_data.drop('label',axis = 1)\n",
    "print('Number of objects = ', small_mnist_data.shape[0])\n",
    "print('Number of features = ',small_mnist_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = pd.DataFrame(load_iris().data)\n",
    "print('Number of objects = ', iris_data.shape[0])\n",
    "print('Number of features = ',iris_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes_data = pd.DataFrame(load_diabetes().data)\n",
    "print('Number of objects = ', diabetes_data.shape[0])\n",
    "print('Number of features = ',diabetes_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston house-prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_data = pd.DataFrame(load_boston().data)\n",
    "print('Number of objects = ', boston_data.shape[0])\n",
    "print('Number of features = ',boston_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olivetti faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "olivetti_data = pd.DataFrame(fetch_olivetti_faces().data)\n",
    "print('Number of objects = ', olivetti_data.shape[0])\n",
    "print('Number of features = ',olivetti_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### California housing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_data = pd.DataFrame(fetch_california_housing().data)\n",
    "print('Number of objects = ', california_data.shape[0])\n",
    "print('Number of features = ',california_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Faces in the Wild (LFW) people dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "lfw_data = pd.DataFrame(fetch_lfw_people(resize = 0.5).data)\n",
    "print('Number of objects = ', lfw_data.shape[0])\n",
    "print('Number of features = ',lfw_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods and Dimensionality Estimation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levina-Bickel Method\n",
    "Original paper: [Levina E., Bickel P.J.: Maximum Likelihood Estimation of Intrinsic Dimension](https://www.stat.berkeley.edu/~bickel/mldim.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "Based on implementation: https://codegists.com/snippet/python/intdim_mlepy_mehdidc_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_dim_sample_wise(X, k=5, dist = None):\n",
    "    \"\"\"\n",
    "    Returns Levina-Bickel dimensionality estimation\n",
    "    \n",
    "    Input parameters:\n",
    "    X    - data\n",
    "    k    - number of nearest neighbours (Default = 5)\n",
    "    dist - matrix of distances to the k nearest neighbors of each point (Optional)\n",
    "    \n",
    "    Returns: \n",
    "    dimensionality estimation for the k \n",
    "    \"\"\"\n",
    "    if dist is None:\n",
    "        neighb = NearestNeighbors(n_neighbors=k+1,algorithm='ball_tree').fit(X)\n",
    "        dist, ind = neighb.kneighbors(X)\n",
    "    dist = dist[:, 1:(k+1)]\n",
    "    assert dist.shape == (X.shape[0], k)\n",
    "    assert np.all(dist > 0)\n",
    "    d = np.log(dist[:, k - 1: k] / dist[:, 0:k - 1])\n",
    "    d = d.sum(axis=1) / (k - 2)\n",
    "    d = 1. / d\n",
    "    intdim_sample = d\n",
    "    return intdim_sample\n",
    "\n",
    "\n",
    "def intrinsic_dim_scale_interval(X, k1=10, k2=20, dist = None):\n",
    "    \"\"\"\n",
    "    Returns range of Levina-Bickel dimensionality estimation for k = k1..k2, k1 < k2\n",
    "    \n",
    "    Input parameters:\n",
    "    X    - data\n",
    "    k1   - minimal number of nearest neighbours (Default = 10)\n",
    "    k2   - maximal number of nearest neighbours (Default = 20)\n",
    "    dist - matrix of distances to the k nearest neighbors of each point (Optional)\n",
    "    \n",
    "    Returns: \n",
    "    list of Levina-Bickel dimensionality estimation for k = k1..k2\n",
    "    \"\"\"\n",
    "    intdim_k = []\n",
    "    if dist is None:\n",
    "        neighb = NearestNeighbors(n_neighbors=k+1,algorithm='ball_tree').fit(X)\n",
    "        dist, ind = neighb.kneighbors(X)\n",
    "        \n",
    "    for k in range(k1, k2 + 1):\n",
    "        m = intrinsic_dim_sample_wise(X, k,dist).mean()\n",
    "        intdim_k.append(m)\n",
    "    return intdim_k\n",
    "\n",
    "\n",
    "def bootstrap_intrinsic_dim_scale_interval(X, nb_iter=100, random_state=None, k1 = 10, k2 = 20, \n",
    "                                           average = True, plot_dependence = False, fig_name = \"\", title_add = \"\"):\n",
    "    \"\"\"\n",
    "    Returns range of Levina-Bickel dimensionality estimation for k = k1..k2 (k1 < k2) averaged over bootstrap samples\n",
    "    \n",
    "    Input parameters:\n",
    "    X            - data\n",
    "    nb_iter      - number of bootstrap iterations (Default = 100)\n",
    "    random_state - random state (Optional)\n",
    "    k1           - minimal number of nearest neighbours (Default = 10)\n",
    "    k2           - maximal number of nearest neighbours (Default = 20)\n",
    "    average      - if False returns array of shape (nb_iter, k2-k1+1) of the estimations for each bootstrap samples (Default = True)\n",
    "    \n",
    "    Returns: \n",
    "    array of shape (k2-k1+1,) of Levina-Bickel dimensionality estimation for k = k1..k2 averaged over bootstrap samples\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        rng = np.random\n",
    "    else:\n",
    "        rng = np.random.RandomState(random_state)\n",
    "    X = pd.DataFrame(X.drop_duplicates().values)\n",
    "    nb_examples = X.shape[0]\n",
    "    results = []\n",
    "    \n",
    "    neighb = NearestNeighbors(n_neighbors=k2+1,algorithm='ball_tree').fit(X)\n",
    "    dist, ind = neighb.kneighbors(X)    \n",
    "    \n",
    "    for i in range(nb_iter):\n",
    "        idx = np.unique(rng.randint(0, nb_examples - 1, size=nb_examples))\n",
    "        results.append(intrinsic_dim_scale_interval(X.iloc[idx], k1, k2, dist[idx,:]))\n",
    "    results = np.array(results)\n",
    "    if plot_dependence:\n",
    "        dim_of_k = results.mean(axis = 0)\n",
    "        levina_dimension = dim_of_k.mean()\n",
    "        print('Dimension averanged over (k=',k1,'..',k2,') = ',levina_dimension)\n",
    "        plt.plot(np.arange(k1,k2+1),dim_of_k)\n",
    "        plt.xlabel('k - nearest neighbours')\n",
    "        plt.ylabel('Dimension')\n",
    "        if title_add == \"\":\n",
    "            plt.title('Original dimension = '+str(X.shape[1])+', L-B dimension = '+str(levina_dimension))\n",
    "        else:\n",
    "            plt.title(title_add + ', original dimension = '+str(X.shape[1])+', L-B dimension = '+str(levina_dimension))\n",
    "        if fig_name != \"\":\n",
    "            plt.savefig(fig_name)\n",
    "    if average:\n",
    "        return results.mean(axis = 0)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k1 = 10 # start of interval(included)\n",
    "k2 = 50 # end of interval(included)\n",
    "f, axarr = plt.subplots(5, 6,figsize=(15,7))\n",
    "for i in range(len(d_sphere_data)):\n",
    "    # dimensionality estimation\n",
    "    dim_of_k = bootstrap_intrinsic_dim_scale_interval(d_sphere_data[i], nb_iter=50, k1=k1, k2=k2)\n",
    "    levina_dimension = round(dim_of_k.mean(),2)\n",
    "    \n",
    "    # plot dependence of dimension estimation from k\n",
    "    axarr[i//6,i%6].plot(np.arange(k1,k2+1),dim_of_k)\n",
    "    axarr[i//6,i%6].set_title(str(i+2)+'D Sphere, d = '+str(levina_dimension))\n",
    "f.tight_layout()\n",
    "plt.savefig(\"pic1_1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Swiss roll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of dimension estimation from k\n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of dimension estimation from k\n",
    "k1 = 10 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to s curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of dimension estimation from k\n",
    "k1 = 3 # start of interval(included)\n",
    "k2 = 20 # end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 5,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 4,20 # start of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Boston house-prices dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Olivetti faces data-set from AT&T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to California housing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start, end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality estimation and ploting dependence of k\n",
    "k1,k2 = 3,20 # start,end of interval(included)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap\n",
    "Original paper: [Joshua B. Tenenbaum,Vin de Silva, John C. Langford: A Global Geometric Framework for Nonlinear Dimensionality Reduction](http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residual_variances_of_isomap(X,dims,n_neighbors = 5, plot_dependence = False, fig_name = \"\", title_add = \"\"):\n",
    "    variances = []\n",
    "    for m in dims:\n",
    "        transformator = Isomap(n_components=m, n_neighbors=n_neighbors)\n",
    "        transformator.fit(X)\n",
    "        X_low = transformator.transform(X)\n",
    "        D = squareform(pdist(X_low)).reshape(-1)\n",
    "        D_fit = transformator.dist_matrix_.reshape(-1)\n",
    "        variances.append(1 - np.corrcoef(D,D_fit)[0,1])\n",
    "    if plot_dependence:\n",
    "        plt.plot(dims, variances,'-^')\n",
    "        plt.xlabel('dimensions')\n",
    "        plt.ylabel('residual variance')\n",
    "        if title_add != \"\":\n",
    "            plt.title(title_add)\n",
    "        if fig_name != \"\":\n",
    "            plt.savefig(fig_name)\n",
    "    return variances\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(5, 6,figsize=(15,7))\n",
    "n_neighbors = 10\n",
    "for i in range(len(d_sphere_data)):\n",
    "    # dimensionality estimation\n",
    "    dims = list(range(1,d_sphere_data[i].shape[1]+1))\n",
    "    var_of_d = compute_residual_variances_of_isomap(d_sphere_data[i], dims, n_neighbors)\n",
    "    \n",
    "    # plot dependence of residual variance from dimension\n",
    "    axarr[i//6,i%6].plot(dims,var_of_d,'-^')\n",
    "    axarr[i//6,i%6].set_title(str(i+2)+'D Sphere')\n",
    "    axarr[i//6,i%6].set_xlabel('d')\n",
    "    axarr[i//6,i%6].set_ylabel('residual variance')\n",
    "f.tight_layout()\n",
    "plt.savefig(\"pic2_1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to S curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Boston house-prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Olivetti faces data-set from AT&T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to LFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Granata-Carnevale method\n",
    "Original paper: [Daniele Granata, Vincenzo Carnevale: Accurate Estimation of the Intrinsic Dimension Using Graph Distances: Unraveling the Geometric Complexity of Datasets](https://www.nature.com/articles/srep31377)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "Implementation is based on: https://github.com/dgranata/Intrinsic-Dimension\n",
    "\n",
    "Беда метода в том, что возможно придётся подгонять r_max по графику (второму), чтоб заработало и на 2d всё ломается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import kneighbors_graph, radius_neighbors_graph\n",
    "from sklearn.utils.graph import graph_shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x,a,b,c):\n",
    "    return a*np.log(np.sin(x/1*np.pi/2.))\n",
    "         \n",
    "def func2(x,a):\n",
    "    return -a/2.*(x-1)**2\n",
    "\n",
    "def func3(x,a,b,c):\n",
    "    return np.exp(c)*np.sin(x/b*np.pi/2.)**a\n",
    "\n",
    "def id_fit(X = None, me = 'euclidean', matrix = False, dist_mat = None, n_neighbors = 3, radius = 0,\n",
    "           n_bins = 50, r_max = 0, r_min = -10, direct = False, projection = False, print_results = False, plot_results = False):\n",
    "    \"\"\"\n",
    "\tNOTE: it is important to have a smooth histogram for accurate fitting.\n",
    "     \n",
    "\t X \t\t\t\t- data, if you have no distance matrix\n",
    "     metric \t\t- define the scipy distance to be used   (Default: euclidean or hamming for MSA)\n",
    "     matrix \t\t- if you want your oun distances between all objects, set matrix to True and enter dist_mat (Default = False)\n",
    "\t dist_mat\t\t- square matrix of distances if matrix == True\n",
    "     n_neighbors \t- nearest_neighbors parameter (Default k=3)\n",
    "     radius \t\t- use neighbor radius instead of nearest_neighbors (Opt, Default = 0)\n",
    "     n_bins \t\t- number of bins for distance histogram (Defaul = 50)\n",
    "     r_max \t\t\t- fix the value of distance distribution maximum in the fit (Opt, Default = 0)\n",
    "     r_min\t\t\t- fix the value of shortest distance considered in the fit \n",
    "\t\t\t\t\t\t(Opt, -1 force the standard fit, avoiding consistency checks, Default = -10)\n",
    "     direct \t\t- analyze the direct (not graph) distances (Opt, Default = False)\n",
    "     projection\t\t- produce an Isomap projection using the first ID components (Opt, Default = False)\n",
    "     print_results  - output results to console or not (Default = False)\n",
    "     plot_results   - plot density estimation, log(r/r_max) vs log(p(r)/p(r_max)), root mean square deviation between \n",
    "                     the observed distribution and the one of a D-dimensional hypersphere as a function of D(RMSD) vs dimension\n",
    "     \n",
    "     returns:\n",
    "     Dfit           - least square fit for p(r) = C(sin(r))^{D-1}\n",
    "     Dmin           - argmin of RMSD\n",
    "     \"\"\"\n",
    "\n",
    "    rmax = r_max\n",
    "    MSA = False\n",
    "    if me == 'hamming':\n",
    "        MSA = True\n",
    "    mm = -10000\n",
    "    input_f = \"results\"\n",
    "\n",
    "    data = X\n",
    "    if matrix: me = 'as from the input file'\n",
    "\n",
    "    if radius > 0.:\n",
    "        filename = str(input_f.split('.')[0]) + 'R' + str(radius)\n",
    "    else:\n",
    "        filename = str(input_f.split('.')[0]) + 'K' + str(n_neighbors)\n",
    "    # 0\n",
    "\n",
    "    # 1 Computing geodesic distance on connected points of the input file and relative histogram\n",
    "    if matrix:\n",
    "        if data.shape[1] == 1:\n",
    "            dist_mat = distance.squareform(data.ravel())\n",
    "            mm = dist_mat.shape[1]\n",
    "        elif data.shape[1] == 3:\n",
    "            mm = int(max(data[:, 1]))\n",
    "            dist_mat = np.zeros((mm, mm))\n",
    "            for i in range(0, data.shape[0]):\n",
    "                dist_mat[int(data[i, 0]) - 1, int(data[i, 1]) - 1] = data[i, 2]\n",
    "                dist_mat[int(data[i, 1]) - 1, int(data[i, 0]) - 1] = data[i, 2]\n",
    "        else:\n",
    "            print('ERROR: The distances input is not in the right matrix format');  sys.exit(2)\n",
    "        if print_results:\n",
    "            print(\"\\n# points: \", mm)\n",
    "\n",
    "        A = np.zeros((mm, mm))\n",
    "        rrr = []\n",
    "\n",
    "        if direct: C = dist_mat\n",
    "        if radius > 0.:\n",
    "            for i in range(0, mm):\n",
    "                ll = dist_mat[i] < radius\n",
    "                A[i, ll] = dist_mat[i, ll]\n",
    "        else:\n",
    "            rrr = np.argsort(dist_mat)\n",
    "            for i in range(0, mm):\n",
    "                ll = rrr[i, 0:n_neighbors + 1]\n",
    "                A[i, ll] = dist_mat[i, ll]\n",
    "            radius = A.max()\n",
    "        C = graph_shortest_path(A, directed=False)\n",
    "\n",
    "    else:\n",
    "        if direct:\n",
    "            C = distance.squareform(distance.pdist(data, me));\n",
    "        elif radius > 0.:\n",
    "            A = radius_neighbors_graph(data, radius, metric=me, mode='distance')\n",
    "            C = graph_shortest_path(A, directed=False)\n",
    "        else:\n",
    "            A = kneighbors_graph(data, n_neighbors, metric=me, mode='distance')\n",
    "            C = graph_shortest_path(A, directed=False)\n",
    "            radius = A.max()\n",
    "\n",
    "    C = np.asmatrix(C)\n",
    "    connect = np.zeros(C.shape[0])\n",
    "    conn = np.zeros(C.shape[0])\n",
    "    for i in range(0, C.shape[0]):\n",
    "        conn_points = np.count_nonzero(C[i])\n",
    "        conn[i] = conn_points\n",
    "        if conn_points > C.shape[0] / 2.:\n",
    "            connect[i] = 1\n",
    "        else:\n",
    "            C[i] = 0\n",
    "    \n",
    "    if np.count_nonzero(connect) > C.shape[0] / 2.:\n",
    "        if print_results:\n",
    "            print('Number of connected points:', np.count_nonzero(connect), '(', 100 * np.count_nonzero(connect) / C.shape[0],\n",
    "                  '% )')\n",
    "    else:\n",
    "        print('The neighbors graph is highly disconnected, increase K or Radius parameters'); sys.exit(2)\n",
    "\n",
    "    indices = np.nonzero(np.triu(C, 1))\n",
    "    dist_list = np.asarray(C[indices])[-1]\n",
    "\n",
    "    h = np.histogram(dist_list, n_bins)\n",
    "    dx = h[1][1] - h[1][0]\n",
    "    \n",
    "    fig,ax = None,None\n",
    "    if plot_results:\n",
    "        fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "        ax[0].plot(h[1][0:n_bins] + dx / 2, h[0], 'o-', label='histogram')\n",
    "        ax[0].set_xlabel('r')\n",
    "        ax[0].set_ylabel('N. counts')\n",
    "        ax[0].legend()\n",
    "    distr_x = []\n",
    "    distr_y = []\n",
    "\n",
    "    avg = np.mean(dist_list)\n",
    "    std = np.std(dist_list)\n",
    "\n",
    "    if rmax > 0:\n",
    "        avg = rmax\n",
    "        if print_results:\n",
    "            print('\\nNOTE: You fixed r_max for the initial fitting, average will have the same value')\n",
    "    else:\n",
    "        mm = np.argmax(h[0])\n",
    "        rmax = h[1][mm] + dx / 2\n",
    "\n",
    "    if r_min >= 0:\n",
    "        if print_results:\n",
    "            print('\\nNOTE: You fixed r_min for the initial fitting: r_min = ', r_min)\n",
    "    if r_min == -1:\n",
    "        if print_results:\n",
    "            print('\\nNOTE: You forced r_min to the standard procedure in the initial fitting')\n",
    "    if print_results:\n",
    "        print('\\nDistances Statistics:')\n",
    "        print('Average, standard dev., n_bin, bin_size, r_max, r_NN_max:', avg, std, n_bins, dx, rmax, radius, '\\n')\n",
    "    # 1\n",
    "    tmp = 1000000\n",
    "    if (r_min >= 0):\n",
    "        tmp = r_min\n",
    "    elif (r_min == -1):\n",
    "        tmp = rmax - std\n",
    "\n",
    "    if (np.fabs(rmax - avg) > std):\n",
    "        print('ERROR: There is a problem with the r_max detection:')\n",
    "        print(\n",
    "        '       usually either the histogram is not smooth enough (you may consider changing the n_bins with option -b)')\n",
    "        print(\n",
    "        '       or r_max and r_avg are too distant and you may consider to fix the first detection of r_max with option -M')\n",
    "        print('       or to change the neighbor parameter with (-r/-k)')\n",
    "        plt.show()\n",
    "        sys.exit()\n",
    "\n",
    "    elif (rmax <= min(radius + dx, tmp)):\n",
    "        print(\n",
    "        'ERROR: There is a problem with the r_max detection, it is shorter than the largest distance in the neighbors graph.')\n",
    "        print(\n",
    "        '       You may consider to fix the first detection of r_max with option -M and/or the r_min with option -n to fix the fit range')\n",
    "        print('       or to decrease the neighbors parameter with (-r/-k)')\n",
    "        plt.show()\n",
    "        sys.exit()\n",
    "\n",
    "    # 2 Finding actual r_max and std. dev. to define fitting interval [rmin;rM]\n",
    "    distr_x = h[1][0:n_bins] + dx / 2\n",
    "    distr_y = h[0][0:n_bins]\n",
    "\n",
    "    res = np.empty(25)\n",
    "    left_distr_x = np.empty(n_bins)\n",
    "    left_distr_y = np.empty(n_bins)\n",
    "    left_distr_x = distr_x[np.logical_and(distr_x[:] > rmax - std, distr_x[:] < rmax + std / 2.0)]\n",
    "    left_distr_y = np.log(distr_y[np.logical_and(distr_x[:] > rmax - std, distr_x[:] < rmax + std / 2.0)])\n",
    "    coeff = np.polyfit(left_distr_x, left_distr_y, 2, full='False')\n",
    "    a0 = coeff[0][0]\n",
    "    b0 = coeff[0][1]\n",
    "    c0 = coeff[0][2]\n",
    "\n",
    "    rmax = -b0 / a0 / 2.0\n",
    "    if (r_max > 0): rmax = r_max\n",
    "    std = np.sqrt(-1 / a0 / 2.)\n",
    "    left_distr_x = distr_x[np.logical_and(distr_x[:] > rmax - std, distr_x[:] < rmax + std / 2.)]\n",
    "    left_distr_y = np.log(distr_y[np.logical_and(distr_x[:] > rmax - std, distr_x[:] < rmax + std / 2.)])\n",
    "    coeff = np.polyfit(left_distr_x, left_distr_y, 2, full='False')\n",
    "    a = coeff[0][0]\n",
    "    b = coeff[0][1]\n",
    "    c = coeff[0][2]\n",
    "\n",
    "    rmax_old = rmax\n",
    "    std_old = std\n",
    "    rmax = -b / a / 2.\n",
    "    std = np.sqrt(-1 / a / 2.)  # it was a0\n",
    "    rmin = max(rmax - 2 * np.sqrt(-1 / a / 2.) - dx / 2, 0.)\n",
    "    if (r_min >= 0):\n",
    "        rmin = r_min\n",
    "    elif (rmin < radius and r_min != -1):\n",
    "        rmin = radius\n",
    "        if print_results:\n",
    "            print(\n",
    "            '\\nWARNING: For internal consistency r_min has been fixed to the largest distance (r_NN_max) in the neighbors graph.')\n",
    "            print(\n",
    "            '         It is possible to reset the standard definition of r_min=r_max-2*sigma running with option \"-n -1\" ')\n",
    "            print('         or you can use -n to manually define a desired value (Example: -n 0.1)\\n')\n",
    "\n",
    "    rM = rmax + dx / 4\n",
    "\n",
    "    if (np.fabs(rmax - rmax_old) > std_old / 4):  # fit consistency check\n",
    "        if print_results:\n",
    "            print(\n",
    "            '\\nWARNING: The histogram is probably not smooth enough (you may try to change n_bin with -b), rmax is fixed to the value of first iteration\\n')\n",
    "            print(rmax,rmax_old,std/4,std_old/4)\n",
    "        rmax = rmax_old\n",
    "        a = a0\n",
    "        b = b0\n",
    "        c = c0\n",
    "        if (r_min >= 0):\n",
    "            rmin = r_min\n",
    "        elif (rmin < radius and r_min != -1):\n",
    "            rmin = radius\n",
    "            if print_results:\n",
    "                print(\n",
    "                '\\nWARNING2: For internal consistency r_min has been fixed to the largest distance in the neighbors graph (r_NN_max).')\n",
    "                print(\n",
    "                '          It is possible to reset the standard definition of r_min=r_max-2*sigma running with option \"-n -1\" ')\n",
    "                print('          or you can use -n to manually define a desired value (Example: -n 0.1)\\n')\n",
    "        rM = rmax + dx / 4\n",
    "    # 2\n",
    "\n",
    "    # 3 Gaussian Fitting to determine ratio R\n",
    "\n",
    "    left_distr_x = distr_x[\n",
    "                       np.logical_and(np.logical_and(distr_x[:] > rmin, distr_x[:] <= rM), distr_y[:] > 0.000001)] / rmax\n",
    "    left_distr_y = np.log(\n",
    "        distr_y[np.logical_and(np.logical_and(distr_x[:] > rmin, distr_x[:] <= rM), distr_y[:] > 0.000001)]) - (\n",
    "                                                                                                             4 * a * c - b ** 2) / 4. / a\n",
    "\n",
    "    fit = curve_fit(func2, left_distr_x, left_distr_y)\n",
    "    ratio = np.sqrt(fit[0][0])\n",
    "    y1 = func2(left_distr_x, fit[0][0])\n",
    "    # 3\n",
    "\n",
    "    # 4 Geodesics D-Hypersphere Distribution Fitting to determine Dfit\n",
    "\n",
    "    fit = curve_fit(func, left_distr_x, left_distr_y)\n",
    "    Dfit = (fit[0][0]) + 1\n",
    "\n",
    "    y2 = func(left_distr_x, fit[0][0], fit[0][1], fit[0][2])\n",
    "    # 4\n",
    "\n",
    "\n",
    "    # 5 Determination of Dmin\n",
    "\n",
    "#     D_file = open('D_residual_{0}.dat'.format(filename), \"w\")\n",
    "\n",
    "    for D in range(1, 26):\n",
    "        y = (func(left_distr_x, D - 1, 1, 0))\n",
    "        for i in range(0, len(y)):\n",
    "            res[D - 1] = np.linalg.norm((y) - (left_distr_y)) / np.sqrt(len(y))\n",
    "#         D_file.write(\"%s \" % D)\n",
    "#         D_file.write(\"%s\\n\" % res[D - 1])\n",
    "\n",
    "    Dmin = np.argmax(-res) + 1\n",
    "\n",
    "    y = func(left_distr_x, Dmin - 1, fit[0][1], 0)\n",
    "    # 5\n",
    "\n",
    "    # 6 Printing results\n",
    "    if print_results:\n",
    "        print('\\nFITTING PARAMETERS:')\n",
    "        print('rmax, std. dev., rmin', rmax, std, rmin)\n",
    "        print('\\nFITTING RESULTS:')\n",
    "        print('R, Dfit, Dmin', ratio, Dfit, Dmin, '\\n')\n",
    "\n",
    "    if plot_results:\n",
    "        ax[1].plot(left_distr_x, left_distr_y, 'o-', label=str(input_f.split('.')[0]))\n",
    "        ax[1].plot(left_distr_x, y1, label='Gaussian fit for R ratio')\n",
    "        ax[1].plot(left_distr_x, y2, label='D-Hypersphere Fit for D_fit')\n",
    "        ax[1].plot(left_distr_x, y, label='D_min-Hypersphere Distribution')\n",
    "        ax[1].set_xlabel('r/r$_{max}$')\n",
    "        ax[1].set_ylabel('log p(r)/p(r$_{max}$)')\n",
    "        ax[1].legend(loc=4)\n",
    "        \n",
    "        ax[2].plot(range(1, 26), res, 'o-', label=str(input_f.split('.')[0]) + ' D_min')\n",
    "        ax[2].legend()\n",
    "        ax[2].set_xlabel('D')\n",
    "        ax[2].set_ylabel('RMDS')\n",
    "#         plt.show()\n",
    "    #     plt.savefig(str(input_f.split('.')[0]) + '_Dmin.png')\n",
    "\n",
    "    # 6\n",
    "\n",
    "    # 7 Optional: Isomap projection\n",
    "    if projection:\n",
    "        from sklearn.decomposition import KernelPCA\n",
    "        C2 = (distance.squareform(dist_list)) ** 2\n",
    "        C2 = -.5 * C2\n",
    "        obj_pj = KernelPCA(n_components=100, kernel=\"precomputed\")\n",
    "        proj = obj_pj.fit_transform(C2)\n",
    "        np.savetxt('proj_' + str(input_f.split('.')[0]) + '.dat', proj[:, 0:Dmin])\n",
    "    return Dfit,Dmin\n",
    "#     print('NOTE: it is important to have a smooth histogram for accurate fitting\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применение к сферам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spheres_dimensions = pd.DataFrame()\n",
    "spheres_dimensions['Sphere dimension'] = list(range(2,len(d_sphere_data)+2))\n",
    "spheres_dimensions['D_min'] = list(range(len(spheres_dimensions)))\n",
    "spheres_dimensions['D_fit'] = np.round(np.linspace(0,1,len(spheres_dimensions)),2)\n",
    "for i in range(1,len(d_sphere_data)):\n",
    "    D_fit,D_min = id_fit(d_sphere_data[i],r_max = 1.5,n_neighbors=10,direct = True,plot_results=False)\n",
    "    spheres_dimensions['D_min'][i] = D_min\n",
    "    spheres_dimensions['D_fit'][i] = np.round(D_fit,2)\n",
    "pd.options.display.max_columns = 30\n",
    "spheres_dimensions.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to S curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применение к Airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применение к MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Applying to Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Applying to Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to Boston house-prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Applying to Olivetti faces data-set from AT&T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Applying to California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Applying to Labeled Faces in the Wild (LFW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценить размерность выборок с помощью PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнить результаты, сделать выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
